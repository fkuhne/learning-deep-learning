

Structuring Machine Learning Projects
Week 2


Autonomous Driving (Case Study)
Graded Quiz. • 1h 15m

DueJul 7, 11:59 PM -03
Try again once you are ready
Grade received 73.33%
Latest Submission Grade 73.33%
To pass 80% or higher
1.
Question 1
To help you practice strategies for machine learning, this week we’ll present another scenario and ask how you would act. We think this “simulator” of working in a machine learning project will give an idea of what leading a machine learning project could be like!

You are employed by a startup building self-driving cars. You are in charge of detecting road signs (stop sign, pedestrian crossing sign, construction ahead sign) and traffic signals (red and green lights) in images. The goal is to recognize which of these objects appear in each image. As an example, this image contains a pedestrian crossing sign and red traffic lights.


Your 100,000 labeled images are taken using the front-facing camera of your car. This is also the distribution of data you care most about doing well on. You think you might be able to get a much larger dataset off the internet, which could be helpful for training even if the distribution of internet data is not the same.

You are getting started with this project. What is the first thing you do? Assume each of the steps below would take about an equal amount of time (a few days).

1 / 1 point

Correct
Applied ML is highly iterative. Having a basic model to do an error analysis can point you in the most promising directions with a lot of certainties.

2.
Question 2
Your goal is to detect road signs (stop sign, pedestrian crossing sign, construction ahead sign) and traffic signals (red and green lights) in images. The goal is to recognize which of these objects appear in each image. You plan to use a deep neural network with ReLU units in the hidden layers.  For the output layer, which of the following gives you the most appropriate activation function?

0 / 1 point

Incorrect
This would be a good choice if only one of the possibilities (stop sign, speed bump, pedestrian crossing, green light, and red light) was present in each image.

3.
Question 3
You are working out error analysis and counting up what errors the algorithm makes. Which of the following do you think you should manually go through and carefully examine, one image at a time?

1 / 1 point

Correct
Correct. We focus on images that the algorithm got wrong from the dev set. That is the one we use to make choices between different iterations of the system.

4.
Question 4
After working on the data for several weeks, your team ends up with the following data: 

100,000 labeled images taken using the front-facing camera of your car. 

900,000 labeled images of roads downloaded from the internet.

Each image’s labels precisely indicate the presence of any specific road signs and traffic signals or combinations of them. For example, 
�
(
�
)
y 
(i)
  = 
[
1
0
0
1
0
]
⎣
⎢
⎢
⎢
⎢
⎢
⎡
​
  
1
0
0
1
0
​
  
⎦
⎥
⎥
⎥
⎥
⎥
⎤
​
  means the image contains a stop sign and a red traffic light.

Because this is a multi-task learning problem, you need to have all your 
�
(
�
)
y 
(i)
  vectors fully labeled. If one example is equal to 
[
0
?
1
1
?
]
⎣
⎢
⎢
⎢
⎢
⎢
⎡
​
  
0
?
1
1
?
​
  
⎦
⎥
⎥
⎥
⎥
⎥
⎤
​
  then the learning algorithm will not be able to use that example. True/False?

1 / 1 point

Correct
As seen in the lecture on multi-task learning, you can compute the cost such that it is not influenced by the fact that some entries haven’t been labeled.

5.
Question 5
The distribution of data you care about contains images from your car’s front-facing camera, which comes from a different distribution than the images you were able to find and download off the internet. Which of the following are true about the train/dev/test split?

1 / 1 point

Correct
Great, you got all the right answers.

6.
Question 6
Assume you’ve finally chosen the following split between the data:


Dataset:

Contains:

Error of the algorithm:

Training

940,000 images randomly picked from (900,000 internet images + 60,000 car’s front-facing camera images)

1%

Training-Dev

20,000 images randomly picked from (900,000 internet images + 60,000 car’s front-facing camera images)

5.1%

Dev

20,000 images from your car’s front-facing camera

5.6%

Test

20,000 images from the car’s front-facing camera

6.8%

You also know that human-level error on the road sign and traffic signals classification task is around 0.5%. Which of the following is true?

1 / 1 point

Correct
Correct. Since the difference between the training-dev error and the training error is high.

7.
Question 7
Assume you’ve finally chosen the following split between the data:

Dataset:

Contains:

Error of the algorithm:

Training

940,000 images randomly picked from (900,000 internet images + 60,000 car’s front-facing camera images)

2%

Training-Dev

20,000 images randomly picked from (900,000 internet images + 60,000 car’s front-facing camera images)

2.3%

Dev

20,000 images from your car’s front-facing camera

1.3%

Test

20,000 images from the car’s front-facing camera

1.1%

You also know that human-level error on the road sign and traffic signals classification task is around 0.5%. Based on the information given, a friend thinks that the training data distribution is much harder than the dev/test distribution. What do you think?

1 / 1 point

Correct
Correct. Since the training-dev error is higher than the dev and test errors, the dev/test distribution is probably "easier" than the training distribution.

8.
Question 8
You decide to focus on the dev set and check by hand what are the errors due to. Here is a table summarizing your discoveries: 

Overall dev set error

15.3%

Errors due to incorrectly labeled data

4.1% 

Errors due to foggy pictures

8.0%

Errors due to rain drops stuck on your car’s front-facing camera

2.2%

Errors due to other causes

1.0%

In this table, 4.1%, 8.0%, etc. are a fraction of the total dev set (not just examples of your algorithm mislabeled). For example, about 8.0/15.3 = 52% of your errors are due to foggy pictures.

The results from this analysis implies that the team’s highest priority should be to bring more foggy pictures into the training set so as to address the 8.0% of errors in that category. True/False?

Additional note: there are subtle concepts to consider with this question, and you may find arguments for why some answers are also correct or incorrect. We recommend that you spend time reading the feedback for this quiz, to understand what issues that you will want to consider when you are building your own machine learning project. 

1 / 1 point

Correct
Correct. This is the correct answer. You should consider the tradeoff between the data accessibility and potential improvement of your model trained on this additional data.

9.
Question 9
You decide to focus on the dev set and check by hand what the errors are due to. Here is a table summarizing your discoveries: 

Overall dev set error

15.3%

Errors due to incorrectly labeled data

4.1% 

Errors due to foggy pictures

3.0%

Errors due to partially occluded elements.

7.2%

Errors due to other causes

1.0%

In this table, 4.1%, 7.2%, etc. are a fraction of the total dev set (not just examples of your algorithm mislabeled). For example, about 7.2/15.3 = 47% of your errors are due to partially occluded elements in the image.

From this table, we can conclude that if we fix the incorrectly labeled data we will reduce the overall dev set error to 11.2%. True/False?

0 / 1 point

Incorrect
The 4.1 only gives you an estimate of the ceiling of how much the error can be improved by fixing the labels.

10.
Question 10
You decide to use data augmentation to address foggy images. You find 1,000 pictures of fog off the internet, and “add” them to clean images to synthesize foggy days, like this:


Which of the following statements do you agree with?

0 / 1 point

Incorrect
11.
Question 11
After working further on the problem, you’ve decided to correct the incorrectly labeled data on the dev set. Which of these statements do you agree with? (Check all that apply).

1 / 1 point

Correct
Great, you got all the right answers.

12.
Question 12
So far your algorithm only recognizes red and green traffic lights. One of your colleagues in the startup is starting to work on recognizing a yellow traffic light. (Some countries call it an orange light rather than a yellow light; we’ll use the US convention of calling it yellow.) Images containing yellow lights are quite rare, and she doesn’t have enough data to build a good model. She hopes you can help her out using transfer learning.  

What do you tell your colleague?

0 / 1 point

Incorrect
13.
Question 13
One of your colleagues at the startup is starting a project to classify stop signs in the road as speed limit signs or not. He has approximately 30,000 examples of each image and 30,000 images without a sign. He thought of using your model and applying transfer learning but then he noticed that you use multi-task learning, hence he can't use your model. True/False?

1 / 1 point

Correct
Correct. When using transfer learning we can remove the last layer. That is one of the aspects that is different from a binary classification problem.

14.
Question 14
When building a system to detect cattle crossing a road from images taken with the front-facing camera of a truck, the designers had a large dataset of images. Which of the following might be a reason to use an end-to-end approach?

1 / 1 point

Correct
Correct. To get good results when using an end-to-end approach, it is necessary to have a big dataset.

15.
Question 15
To recognize a stop sign you use the following approach:  

First, we localize any traffic sign in an image. After that, we determine if the sign is a stop sign or not.  

This is a better approach than an end-to-end model for which of the following cases? Choose the best answer.

1 / 1 point

Correct
Correct. This might be the most important factor when deciding whether to use an end-to-end approach.

